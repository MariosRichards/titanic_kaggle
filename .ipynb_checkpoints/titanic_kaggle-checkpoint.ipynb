{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The Competition\n",
    "We'll be learning how to generate a submission for a Kaggle competition. Kaggle is a site where you create algorithms, and compete against machine learning practitioners around the world. Your algorithm wins if it's the most accurate on a given dataset. Kaggle is a fun way to practice your machine learning skills.\n",
    "\n",
    "Kaggle has several different competitions on their site. On of them is about predicting which passengers survived the sinking of the Titanic. In this and the next mission, we'll be learning how to make our first submission to the competition.\n",
    "\n",
    "Our data is in .csv format. You can get started with the competition and download the data here.\n",
    "https://www.kaggle.com/c/titanic/download/train.csv\n",
    "(save as titanic_train.csv and put in code folder!)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PassengerId   -- A numerical id assigned to each passenger.\n",
    "Survived      -- Whether the passenger survived (1), or didn't (0). We'll be making predictions for this column.\n",
    "Pclass        -- The class the passenger was in -- first class (1), second class (2), or third class (3).\n",
    "Name          -- the name of the passenger.\n",
    "Sex           -- The gender of the passenger -- male or female.\n",
    "Age           -- The age of the passenger. Fractional.\n",
    "SibSp         -- The number of siblings and spouses the passenger had on board.\n",
    "Parch         -- The number of parents and children the passenger had on board.\n",
    "Ticket        -- The ticket number of the passenger.\n",
    "Fare          -- How much the passenger paid for the ticker.\n",
    "Cabin         -- Which cabin the passenger was in.\n",
    "Embarked      -- Where the passenger boarded the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "# https://www.dataquest.io/mission/74/getting-started-with-kaggle/\n",
    "# Getting Started With Kaggle\n",
    "titanic = pandas.read_csv(\"titanic_train.csv\")\n",
    "print(titanic.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n"
     ]
    }
   ],
   "source": [
    "print(titanic.head(5))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "When you used .describe() on the titanic dataframe in the last screen, you might have noticed that the Age column has a count of 714 when all the other columns have a count of 891. This indicates that there are missing values in the Age column -- the count is of non-missing (null, NA, or not a number) values.\n",
    "\n",
    "Tutorial suggests - replace with median (simple but ... so wrong!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  891.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.361582    0.523008   \n",
      "std     257.353842    0.486592    0.836071   13.019697    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   22.000000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   35.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n"
     ]
    }
   ],
   "source": [
    "titanic[\"Age\"] = titanic[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "print(titanic.describe())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Non-Numeric Columns\n",
    "When we used .describe() two screens ago, you might have also noticed that not all the columns were shown. Only the numeric columns were shown. Several of our columns are non-numeric, which is a problem when it comes time to make predictions -- we can't feed non-numeric columns into a machine learning algorithm and expect it to make sense of them.\n",
    "\n",
    "We have to either exclude our non-numeric columns when we train our algorithm (Name, Sex, Cabin, Embarked, and Ticket), or find a way to convert them to numeric columns.\n",
    "\n",
    "We'll ignore the Ticket, Cabin, and Name columns. There isn't much information we can extract from there. Most of the values in the cabin column are missing (only 204 values out of 891 rows), and it likely isn't a particularly informative column in the first place. The Ticket and Name columns are unlikely to tell us much without some domain knowledge about what the ticket numbers mean, and about which names correlate with characteristics like large or rich families."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Converting The Sex Column\n",
    "The Sex column is non-numeric, but we want to keep it around -- it could be very informative. We can convert it to a numeric column by replacing each gender with a numeric code. A machine learning algorithm will then be able to use these categories to make predictions.\n",
    "\n",
    "To do this, we first have to find all the unique genders in the column (we know male and female are there, but did whoever recorded the dataset use another code for missing values?). We'll also assign a code of 0 to male, and a code of 1 to female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male' 'female']\n"
     ]
    }
   ],
   "source": [
    "# Find all the unique genders -- the column appears to contain only male and female.\n",
    "print(titanic[\"Sex\"].unique())\n",
    "\n",
    "# Replace all the occurences of male with the number 0.\n",
    "titanic.loc[titanic[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "titanic.loc[titanic[\"Sex\"] == \"female\", \"Sex\"] = 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Converting The Embarked Column\n",
    "We now can convert the Embarked column to codes the same way we converted the Sex column. The unique values in Embarked are S, C, Q, and missing (nan). Each letter is an abbreviation of an embarkation port name.\n",
    "We'll assign the code 0 to S, 1 to C and 2 to Q. Replace each value in the Embarked column with its corresponding code.\n",
    "\n",
    "Tutoral suggests - replace with mode - also, pretty crude!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S' 'C' 'Q' nan]\n",
      "0    S\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Find all the unique values for \"Embarked\".\n",
    "print(titanic[\"Embarked\"].unique())\n",
    "print(titanic[\"Embarked\"].mode())\n",
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna(\"S\")\n",
    "titanic.loc[titanic[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic.loc[titanic[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic.loc[titanic[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On To Machine Learning!\n",
    "If we wanted to make predictions about whether someone survived or not from the Age column, we could use a technique called linear regression. Linear regression follows the equation y=mx+by=mx+b, where y is the value we're trying to predict, m is a coefficient called the slope, x is the value of a column, and b is a constant called the intercept."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Cross Validation\n",
    "We can now use linear regression to make predictions on our training set.\n",
    "\n",
    "We want to train the algorithm on different data than we make predictions on. This is critical if we want to avoid overfitting. Overfitting is what happens when a model fits itself to \"noise\", not signal. Every dataset has its own quirks that don't exist in the full population. For example, if I asked you to predict the top speed of a car from its horsepower and other characteristics, and gave you a dataset that randomly had cars with very high top speeds, you would create a model that overstated speed. The way to figure out if your model is doing this is to evaluate its performance on data it hasn't been trained using.\n",
    "\n",
    "Every machine learning algorithm can overfit, although some (like linear regression) are much less prone to it. If you evaluate your algorithm on the same dataset that you train it on, it's impossible to know if it's performing well because it overfit itself to the noise, or if it actually is a good algorithm.\n",
    "\n",
    "Luckily, cross validation is a simple way to avoid overfitting. To cross validate, you split your data into some number of parts (or \"folds\"). Lets use 3 as an example. You then do this:\n",
    "\n",
    "Combine the first two parts, train a model, make predictions on the third.\n",
    "\n",
    "Combine the first and third parts, train a model, make predictions on the second.\n",
    "\n",
    "Combine the second and third parts, train a model, make predictions on the first.\n",
    "\n",
    "This way, we generate predictions for the whole dataset without ever evaluating accuracy on the same data we train our model using."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can use the excellent scikit-learn library to make predictions. We'll use a helper from sklearn to split the data up into cross validation folds, and then train an algorithm for each fold, and make predictions. At the end, we'll have a list of predictions, with each list item containing predictions for the corresponding fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the linear regression class\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Sklearn also has a helper that makes it easy to do cross validation\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "# The columns we'll use to predict the target\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm class\n",
    "alg = LinearRegression()\n",
    "# Generate cross validation folds for the titanic dataset.  It return the row indices corresponding to train and test.\n",
    "# We set random_state to ensure we get the same splits every time we run this.\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    # The predictors we're using the train the algorithm.  Note how we only take the rows in the train folds.\n",
    "    train_predictors = (titanic[predictors].iloc[train,:])\n",
    "    # The target we're using to train the algorithm.\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    # Training the algorithm using the predictors and target.\n",
    "    alg.fit(train_predictors, train_target)\n",
    "    # We can now make predictions on the test fold\n",
    "    test_predictions = alg.predict(titanic[predictors].iloc[test,:])\n",
    "    predictions.append(test_predictions)\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "# The predictions are in three separate numpy arrays.  Concatenate them into one.  \n",
    "# We concatenate them on axis 0, as they only have one axis.\n",
    "predictions = np.concatenate(predictions, axis=0)    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Evaluating Error\n",
    "Now that we have predictions, we can evaluate our error.\n",
    "\n",
    "We'll first need to define an error metric, so we can figure out how accurate our model is. From the Kaggle competition description, the error metric is percentage of correct predictions. We'll use this same metric to evaluate our performance locally.\n",
    "\n",
    "The metric will basically involve finding the number of values in predictions that are the exact same as their counterparts in titanic[\"Survived\"], and then dividing by the total number of passengers.\n",
    "\n",
    "Before we can do this, we need to combine the 3 sets of predictions into one column. Since each set of predictions is a numpy (python scientific computing library) array, we can use a numpy function to concatenate them into one.\n",
    "\n",
    "Note! Percentage represented as a fraction of 1 not a fraction of 100!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.783389450056\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Map predictions to outcomes (only possible outcomes are 1 and 0)\n",
    "predictions[predictions > .5] = 1\n",
    "predictions[predictions <=.5] = 0\n",
    "\n",
    "#accuracy = predictions\n",
    "#titanic[\"Survived\"]\n",
    "accuracy = sum(predictions == titanic[\"Survived\"]) / len(predictions)\n",
    "\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Logistic Regression\n",
    "We have our first predictions! They aren't very good, though, with only 78.3% accuracy. In the video, we mentioned a way to make linear regression output values between 0 and 1. This is a technique called logistic regression.\n",
    "\n",
    "One good way to think of logistic regression is that it takes the output of a linear regression, and maps it so it is between 0 and 1. The does this with the logit function. Passing any value through the logit function will map it to a value between 0 and 1 by \"squeezing\" the extreme values. This is perfect for us, because we only care about two outcomes.\n",
    "\n",
    "Sklearn has a class for logistic regression that we can use. We'll also make things easier by using an sklearn helper function to do all of our cross validation and evaluation for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787878787879\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize our algorithm\n",
    "alg = LogisticRegression(random_state=1)\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Processing The Test Set\n",
    "Our accuracy is decent, but not great. We can still try a few things to make it better, which we'll talk about in the next mission.\n",
    "\n",
    "But, we need to make a submission to the competition. To do this, we need to take the exact same steps on the test data that we took on the training data. If we don't do the exact same operations, then we won't be able to make valid predictions on it.\n",
    "\n",
    "These operations are all the changes we made to the columns before.\n",
    "\n",
    "\n",
    "https://www.kaggle.com/c/titanic/download/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "#https://www.kaggle.com/c/titanic/download/test.csv\n",
    "titanic_test = pandas.read_csv(\"titanic_test.csv\")\n",
    "# use the median age from the traing set\n",
    "titanic_test[\"Age\"] = titanic_test[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "# extra column with some missing empty in the test set\n",
    "titanic_test[\"Fare\"] = titanic_test[\"Fare\"].fillna(titanic_test[\"Fare\"].median())\n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "titanic_test[\"Embarked\"] = titanic_test[\"Embarked\"].fillna(\"S\")\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n",
    "\n",
    "    \n",
    "# Initialize the algorithm class\n",
    "alg = LogisticRegression(random_state=1)\n",
    "\n",
    "# Train the algorithm using all the training data\n",
    "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Make predictions using the test set.\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "# Create a new dataframe with only the columns Kaggle wants from the dataset.\n",
    "submission = pandas.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "    \n",
    "submission.to_csv(\"kaggle.csv\", index=False)    \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Implementing A Random Forest\n",
    "Thankfully for us, sklearn has a nice random forest implementation already. We can use it to construct a random forest and generate cross validated predictions on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.801346801347\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm with the default paramters\n",
    "# n_estimators is the number of trees we want to make\n",
    "# min_samples_split is the minimum number of rows we need to make a split\n",
    "# min_samples_leaf is the minimum number of samples we can have at the place where a tree branch ends (the bottom points of the tree)\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)\n",
    "\n",
    "\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Parameter Tuning\n",
    "The first, and easiest, thing we can do to improve the accuracy of the random forest is to increase the number of trees we're using. Training more trees will take more time, but because of the fact that we're averaging many predictions made on different subsets of the data, having more trees will increase accuracy greatly (up to a point).\n",
    "\n",
    "We can also tweak the min_samples_split and min_samples_leaf variables to reduce overfitting. Because of how a decision tree works (as we explained in the video), having splits that go all the way down, or overly deep in the tree can result in fitting to quirks in the dataset, and not true signal. Thus, increasing min_samples_split and min_samples_leaf can reduce overfitting, which will actually improve our score, as we're making predictions on unseen data. A model that is less overfit, and that can generalize better, will actually perform better on unseen data, but worse on seen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.820426487093\n"
     ]
    }
   ],
   "source": [
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Generating New Features\n",
    "We can also generate new features. Here are some ideas:\n",
    "\n",
    "The length of the name -- this could pertain to how rich the person was, and therefore their position in the Titanic.\n",
    "The total number of people in a family (SibSp + Parch).\n",
    "An easy way to generate features is to use the .apply method on pandas dataframes. This applies a function you pass in to each element in a dataframe or series. We can pass in a lambda function, which enables us to define a function inline.\n",
    "\n",
    "To write a lambda function, you write lambda x: len(x). x will take on the value of the input that is passed in -- in this case, the passenger name. The function to the right of the colon is then applied to x, and the result returned. The .apply method takes all of these outputs and constructs a pandas series from them. We can assign this series to a dataframe column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating a familysize column\n",
    "titanic[\"FamilySize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"]\n",
    "\n",
    "# The .apply method generates a new series\n",
    "titanic[\"NameLength\"] = titanic[\"Name\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using The Title\n",
    "We can extract the title of the passenger from their name. The titles take the form of Master., Mr., Mrs.. There are a few very commonly used titles, and a \"long tail\" of one-off titles that only one or two passengers have.\n",
    "\n",
    "We'll first extract the titles with a regular expression, and then map each unique title to an integer value.\n",
    "\n",
    "We'll then have a numeric column that corresponds to the appropriate Title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr          517\n",
      "Miss        182\n",
      "Mrs         125\n",
      "Master       40\n",
      "Dr            7\n",
      "Rev           6\n",
      "Major         2\n",
      "Mlle          2\n",
      "Col           2\n",
      "Countess      1\n",
      "Sir           1\n",
      "Ms            1\n",
      "Lady          1\n",
      "Jonkheer      1\n",
      "Mme           1\n",
      "Don           1\n",
      "Capt          1\n",
      "Name: Name, dtype: int64\n",
      "1     517\n",
      "2     183\n",
      "3     125\n",
      "4      40\n",
      "5       7\n",
      "6       6\n",
      "7       5\n",
      "10      3\n",
      "8       3\n",
      "9       2\n",
      "Name: Name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# A function to get the title from a name.\n",
    "def get_title(name):\n",
    "    # Use a regular expression to search for a title.  Titles always consist of capital and lowercase letters, and end with a period.\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "# Get all the titles and print how often each one occurs.\n",
    "titles = titanic[\"Name\"].apply(get_title)\n",
    "print(pandas.value_counts(titles))\n",
    "\n",
    "# Map each title to an integer.  Some titles are very rare, and are compressed into the same codes as other titles.\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "\n",
    "# Verify that we converted everything.\n",
    "print(pandas.value_counts(titles))\n",
    "\n",
    "# Add in the title column.\n",
    "titanic[\"Title\"] = titles"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Family Groups\n",
    "We can also generate a feature indicating which family people are in. Because survival was likely highly dependent on your family and the people around you, this has a good chance at being a good feature.\n",
    "\n",
    "To get this, we'll concatenate someone's last name with FamilySize to get a unique family id. We'll then be able to assign a code to each person based on their family id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1      800\n",
      " 14       8\n",
      " 149      7\n",
      " 63       6\n",
      " 50       6\n",
      " 59       6\n",
      " 17       5\n",
      " 384      4\n",
      " 27       4\n",
      " 25       4\n",
      " 162      4\n",
      " 8        4\n",
      " 84       4\n",
      " 340      4\n",
      " 43       3\n",
      " 269      3\n",
      " 58       3\n",
      " 633      2\n",
      " 167      2\n",
      " 280      2\n",
      " 510      2\n",
      " 90       2\n",
      " 83       1\n",
      " 625      1\n",
      " 376      1\n",
      " 449      1\n",
      " 498      1\n",
      " 588      1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "# A dictionary mapping family name to id\n",
    "family_id_mapping = {}\n",
    "\n",
    "# A function to get the id given a row\n",
    "def get_family_id(row):\n",
    "    # Find the last name by splitting on a comma\n",
    "    last_name = row[\"Name\"].split(\",\")[0]\n",
    "    # Create the family id\n",
    "    family_id = \"{0}{1}\".format(last_name, row[\"FamilySize\"])\n",
    "    # Look up the id in the mapping\n",
    "    if family_id not in family_id_mapping:\n",
    "        if len(family_id_mapping) == 0:\n",
    "            current_id = 1\n",
    "        else:\n",
    "            # Get the maximum id from the mapping and add one to it if we don't have an id\n",
    "            current_id = (max(family_id_mapping.items(), key=operator.itemgetter(1))[1] + 1)\n",
    "        family_id_mapping[family_id] = current_id\n",
    "    return family_id_mapping[family_id]\n",
    "\n",
    "# Get the family ids with the apply method\n",
    "family_ids = titanic.apply(get_family_id, axis=1)\n",
    "\n",
    "# There are a lot of family ids, so we'll compress all of the families under 3 members into one code.\n",
    "family_ids[titanic[\"FamilySize\"] < 3] = -1\n",
    "\n",
    "# Print the count of each unique id.\n",
    "print(pandas.value_counts(family_ids))\n",
    "\n",
    "titanic[\"FamilyId\"] = family_ids"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Finding The Best Features\n",
    "Feature engineering is the most important part of any machine learning task, and there are lots more features we could calculate. But we also need a way to figure out which features are the best.\n",
    "\n",
    "One way to do this is to use univariate feature selection. This essentially goes column by column, and figures out which columns correlate most closely with what we're trying to predict (Survived).\n",
    "\n",
    "As usual, sklearn has a function that will help us with feature selection, SelectKBest. This selects the best features from the data, and allows us to specify how many it selects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAEsCAYAAAAIBeLrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xaec16a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.811447811448\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "# Plot the scores.  See how \"Pclass\", \"Sex\", \"Title\", and \"Fare\" are the best?\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "# Pick only the four best features.\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Title\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=8, min_samples_leaf=4)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Gradient Boosting\n",
    "\n",
    "Another method that builds on decision trees is a gradient boosting classifier. Boosting involves training decision trees one after another, and feeding the errors from one tree into the next tree. So each tree is building on all the other trees that came before it. This can lead to overfitting if we build too many trees, though. As you get above 100 trees or so, it's very easy to overfit and train to quirks in the dataset. As our dataset is extremely small, we'll limit the tree count to just 25.\n",
    "\n",
    "Another way to limit overfitting is to limit the depth to which each tree in the gradient boosting process can be built. We'll limit the tree depth to 3 to avoid overfitting.\n",
    "\n",
    "We'll try boosting instead of our random forest approach and see if we can improve our accuracy."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ensembling\n",
    "\n",
    "One thing we can do to improve the accuracy of our predictions is to ensemble different classifiers. Ensemblingh means that we generate predictions using information from a set of classifiers, instead of just one. In practice, this means that we average their predictions.\n",
    "\n",
    "Generally, the more diverse the models we ensemble, the higher our accuracy will be. Diversity means that the models generate their results from different columns, or use a very different method to generate predictions. Ensembling a random forest classifier with a decision tree probably won't work extremely well, because they are very similar. On the other hand, ensembling a linear regression with a random forest can work very well.\n",
    "\n",
    "One caveat with ensembling is that the classifiers we use have to be about the same in terms of accuracy. Ensembling one classifier that is much worse than another probably will make the final result worse.\n",
    "\n",
    "In this case, we'll ensemble logistic regression trained on the most linear predictors (the ones that have a linear ordering, and some correlation to Survived), and a gradient boosted tree trained on all of the predictors.\n",
    "\n",
    "We'll keep things simple when we ensemble -- we'll average the raw probabilities (from 0 to 1) that we get from our classifiers, and then assume that anything above .5 maps to one, and anything below or equal to .5 maps to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.819304152637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\titanic_kaggle\\lib\\site-packages\\ipykernel\\__main__.py:37: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import numpy as np\n",
    "\n",
    "# The algorithms we want to ensemble.\n",
    "# We're using the more linear predictors for the logistic regression, and everything with the gradient boosting classifier.\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "# Initialize the cross validation folds\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm on the training data.\n",
    "        alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
    "        # Select and predict on the test fold.  \n",
    "        # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
    "        test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
    "    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# Put all the predictions together into one array.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Compute accuracy by comparing to the training data.\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Matching Our Changes On The Test Set\n",
    "There are a lot of things we could do to make this analysis better that we'll talk about at the end, but for now, let's make a submission.\n",
    "\n",
    "The first step is matching all our training set changes on the test set data, like we did in the last mission. We've read the test set into titanic_test. We'll have to match our changes:\n",
    "\n",
    "Generate the NameLength column, which is how long the name is.\n",
    "Generate the FamilySize column, showing how large a family is.\n",
    "Add in the Title column, keeping the same mapping that we had before.\n",
    "Add in a FamilyId column, keeping the ids consistent across the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     240\n",
      "2      79\n",
      "3      72\n",
      "4      21\n",
      "7       2\n",
      "6       2\n",
      "10      1\n",
      "5       1\n",
      "Name: Title, dtype: int64\n",
      "{'Madsen0': 119, 'Karlsson0': 410, 'Kalvik0': 534, 'McCormack0': 661, 'Baumann0': 156, 'Garside0': 481, 'Doling1': 95, 'de Pelsmaeker0': 255, 'Appleton2': 478, 'Alexander0': 650, 'Chaffee1': 89, 'Lindblom0': 250, 'Ryerson4': 280, 'Swift0': 682, 'Gallagher0': 573, 'Vovk0': 442, 'Leitch0': 496, 'Jardin0': 507, 'Daly0': 432, 'Kantor1': 96, 'Pinsky0': 174, 'Lemore0': 437, 'Lam0': 565, 'Connaghton0': 605, 'Silvey1': 375, 'Icard0': 61, 'Faunthorpe1': 53, 'Maenpaa0': 221, 'Carter3': 340, 'Rommetvedt0': 545, 'Ahlin1': 40, 'Graham1': 242, 'Dennis0': 288, 'Brown0': 177, 'Moutal0': 75, 'Patchett0': 480, 'Bissette0': 243, 'Somerton0': 416, 'Leyson0': 213, 'White1': 99, 'Cacic0': 405, 'Nysveen0': 292, 'Bjornstrom-Steffansson0': 371, 'Harmer0': 634, 'Mamee0': 36, 'Parkes0': 251, 'Gilnagh0': 146, 'Gustafsson2': 101, 'Roebling0': 686, 'Knight0': 593, 'Laitinen0': 427, 'Seward0': 383, 'Backstrom1': 188, \"O'Sullivan0\": 426, 'Nankoff0': 598, 'Hays2': 658, 'Henry0': 239, 'del Carlo1': 316, 'LeRoy0': 452, 'Uruchurtu0': 30, 'Keefe0': 404, 'Rothschild1': 434, 'Lang0': 431, 'Scanlan0': 403, 'Goodwin7': 59, 'Duff Gordon1': 467, 'Cribb1': 150, 'Hansen2': 680, 'Naidenoff0': 259, 'Cherry0': 234, 'Potter1': 692, 'Torber0': 501, 'Collander0': 301, 'Johnston3': 633, 'Rice5': 17, 'Isham0': 163, 'Lahoud0': 443, 'Zimmerman0': 364, 'Gustafsson0': 331, 'Ringhini0': 327, 'Hocking3': 449, 'Stephenson1': 493, 'Bidois0': 332, 'Sirayanian0': 60, 'Givard0': 195, 'Fortune5': 27, 'Bradley0': 430, 'Moore0': 116, \"O'Dwyer0\": 28, 'Pears1': 141, 'Gillespie0': 585, 'Goncalves0': 400, 'Thorneycroft1': 372, 'Buss0': 337, 'Yousif0': 310, 'Drew2': 358, 'Davis0': 525, 'Hart0': 353, 'Crosby2': 455, 'Frost0': 412, 'Olsen0': 144, 'Drazenoic0': 122, 'Coelho0': 123, 'Warren1': 320, 'Butler0': 544, 'Mullens0': 569, 'Downton0': 485, 'Nakid2': 333, 'Ekstrom0': 121, 'Vander Planke2': 38, 'Saundercock0': 13, 'Otter0': 640, 'Behr0': 700, 'Corn0': 147, 'Strom1': 187, 'Elias2': 309, 'Abbing0': 675, 'Ibrahim Shawah0': 643, 'Farthing0': 447, 'Bing0': 72, 'Paulner0': 487, 'West3': 58, \"O'Driscoll0\": 47, 'Robbins0': 468, 'Moor1': 607, 'Rouse0': 413, 'Bonnell0': 12, 'Hanna0': 268, 'Aubart0': 323, 'Dowdell0': 77, 'Murdlin0': 491, 'Brewe0': 619, 'Futrelle1': 4, 'Stewart0': 64, 'Peuchen0': 385, 'Coleff0': 435, 'Walker0': 436, 'Dantcheff0': 639, 'Pengelly0': 217, 'Nirva0': 615, 'Harris0': 201, 'Sjoblom0': 635, 'Youseff0': 185, 'Moen0': 73, 'Aks1': 678, 'Trout0': 344, 'Farrell0': 445, 'Caldwell2': 76, 'Hassan0': 592, 'Porter0': 107, 'Wheadon0': 33, 'Mernagh0': 179, 'Smiljanic0': 148, 'Maisner0': 399, 'Pernot0': 166, 'Strandberg0': 407, 'Garfirth0': 614, 'Betros0': 330, 'Kent0': 415, 'Cardeza1': 556, 'Allison3': 269, 'Thayer2': 461, 'Johnson2': 9, 'Chibnall1': 155, 'Murphy1': 219, 'Ryan0': 438, 'Lurette0': 178, 'Bengtsson0': 152, 'Nicholls2': 136, 'Sedgwick0': 302, 'Jacobsohn3': 498, 'Troupiansky0': 595, 'McMahon0': 118, 'McGovern0': 314, 'Odahl0': 307, 'Sharp0': 462, 'Levy0': 264, 'Hendekovic0': 282, 'Sagesser0': 528, 'Bishop1': 263, 'Shelley1': 693, 'Bostandyeff0': 519, 'Foreman0': 387, 'Bracken0': 203, 'Thomas1': 645, 'Risien0': 453, 'Ford4': 84, 'Hold1': 215, 'Dodge2': 382, 'Parr0': 524, 'Barah0': 616, 'Francatelli0': 278, 'Thorne0': 233, 'Elias0': 624, 'Hood0': 70, 'Edvardsson0': 553, 'Morley0': 396, 'Birkeland0': 351, 'Angle1': 439, 'Jarvis0': 488, 'Novel0': 57, 'Smart0': 402, 'Mitkoff0': 533, 'Hocking4': 625, 'Caram1': 482, 'Olsson0': 254, 'Maioni0': 428, 'Kraeff0': 42, 'Hassab0': 558, 'Montvila0': 698, 'Doharr0': 476, 'Newell1': 197, 'Fox0': 303, 'Hamalainen2': 224, 'Newell2': 539, 'Karun1': 564, 'Rugg0': 56, 'Moraweck0': 285, 'Hart2': 283, 'Celotti0': 86, 'Frolicher2': 454, 'Danoff0': 289, 'Barbara1': 317, 'McGough0': 433, 'Humblen0': 570, 'Perreault0': 441, 'Nosworthy0': 51, 'Gronnestad0': 621, 'Sloper0': 24, 'Ali0': 192, 'Wright0': 466, 'Guggenheim0': 636, 'Meanwell0': 474, 'Mallet2': 656, 'Yasbeck1': 512, 'Astor1': 571, 'Jussila1': 110, 'Rothes0': 613, 'Larsson0': 211, 'Osen0': 129, 'Rekic0': 105, 'Niskanen0': 345, \"O'Brien0\": 463, 'Slayter0': 290, 'Giles1': 681, 'Duran y More1': 685, 'Nysten0': 132, 'Banfield0': 696, 'Barkworth0': 521, 'Frauenthal1': 296, 'Abbott2': 252, 'Peters0': 557, 'Penasco y Castellana1': 276, 'Dahl0': 299, 'Nenkoff0': 205, 'Richard0': 127, 'Yrois0': 182, 'Emanuel0': 628, 'Blank0': 191, 'Gavey0': 511, 'Palsson4': 8, 'Green0': 204, 'Van Impe2': 361, 'Mannion0': 589, 'Goldsmith2': 154, 'Lester0': 651, 'Lindell1': 503, 'Norman0': 472, 'Sheerlinck0': 79, 'Hegarty0': 536, 'Simmons0': 473, 'Sundman0': 356, 'Hoyt1': 206, 'Beesley0': 22, 'Sawyer0': 554, 'Vander Planke1': 19, 'Laleff0': 691, 'Fry0': 654, 'Hansen0': 514, 'Jalsevac0': 390, 'Webber0': 117, 'Fischer0': 561, 'Long0': 632, 'Widener2': 329, 'Lehmann0': 339, 'Soholt0': 580, 'Elsbury0': 494, 'Christy2': 484, 'Daniel0': 505, 'Simonius-Blumer0': 531, 'Chip0': 668, 'Klasen2': 161, 'Hirvonen1': 411, 'Stankovic0': 257, 'Berriman0': 594, 'Peduzzi0': 389, 'Windelov0': 417, 'Wick2': 286, 'Hippach1': 294, 'Lievens0': 622, 'Mellors0': 208, 'Andersen-Jensen1': 176, 'Stranden0': 602, 'Meyer0': 649, 'Healy0': 248, 'Denkoff0': 297, 'Allen0': 5, 'Pekoniemi0': 112, 'Cook0': 546, 'Honkanen0': 198, 'Foo0': 529, 'Clifford0': 408, 'Reed0': 227, 'Perkin0': 194, 'Andrew0': 135, 'Morrow0': 470, 'Mack0': 623, 'Andrews0': 647, 'Hogeboom1': 618, 'Wiseman0': 366, 'Calic0': 153, 'Lundahl0': 522, 'Plotcharsky0': 335, 'Lemberopolous0': 673, 'Todoroff0': 29, 'Coutts2': 305, 'Cairns0': 244, 'Carrau0': 81, 'Markoff0': 676, 'Herman3': 510, 'Dean3': 90, 'Allum0': 664, 'Brocklebank0': 509, 'Klaber0': 578, 'McKane0': 342, 'Bystrom0': 684, 'Pain0': 343, 'Landergren0': 328, 'Richards5': 376, 'Duane0': 253, 'Sivola0': 159, 'Frolicher-Stehli2': 489, 'Eitemiller0': 538, 'Badt0': 541, 'Lesurer0': 596, 'Keane0': 274, 'Staneff0': 74, \"O'Connell0\": 520, 'McEvoy0': 583, 'Hagland1': 386, 'Williams1': 145, 'Spedden2': 287, 'Touma2': 232, 'Albimona0': 189, 'Williams0': 18, 'Blackwell0': 300, 'Chambers1': 587, \"O'Leary0\": 535, 'Stahelin-Maeglin0': 523, 'Jonsson0': 477, 'Glynn0': 32, 'Hewlett0': 16, 'Harknett0': 214, 'Woolner0': 55, 'Moss0': 104, 'Carter1': 226, 'Ponesell0': 644, 'Vander Cruyssen0': 689, 'Navratil2': 138, 'Robins1': 124, 'Shorney0': 92, 'Cavendish1': 600, 'Hedman0': 646, 'Alhomaki0': 670, 'Greenberg0': 579, 'Wiklund1': 325, 'Hunt0': 218, 'Nicholson0': 458, 'Johansson0': 100, 'Olsen1': 180, 'Heininen0': 655, 'Compton2': 665, 'Shutes0': 506, 'Lindqvist1': 543, 'Moussa0': 321, 'Strom2': 228, 'Homer0': 502, 'Silven2': 359, 'Masselmani0': 20, 'Beavan0': 326, 'Slocovski0': 85, 'Ivanoff0': 597, 'van Billiard2': 143, 'Andreasson0': 88, 'Harrison0': 238, 'Holm0': 657, 'Turpin1': 41, 'Asplund6': 25, 'Gill0': 683, 'Culumovic0': 674, 'Mineff0': 266, 'Theobald0': 612, 'Vanden Steen0': 311, 'Harder1': 324, 'Barber0': 262, 'Emir0': 26, 'Dooley0': 701, 'Persson1': 241, 'Leeni0': 464, 'Rush0': 479, 'McNamee1': 601, 'Calderhead0': 575, 'Hickman2': 115, 'Turja0': 555, 'Leonard0': 165, 'Leader0': 641, 'Augustsson0': 663, 'Fahlstrom0': 210, 'Serepeca0': 672, 'Partner0': 295, 'Rogers0': 45, 'Weir0': 567, 'Cameron0': 193, 'Chapman1': 495, 'Kallio0': 374, 'Pettersson0': 648, 'Osman0': 642, 'Taussig2': 237, 'Collyer2': 216, 'Sutton0': 516, 'Hays0': 279, 'Sandstrom2': 11, 'Toufik0': 450, 'Nicola-Yarred1': 39, 'Kassem0': 444, 'Braund1': 1, 'Lewy0': 267, 'Lefebre4': 162, 'Dahlberg0': 695, 'Carbines0': 175, 'Vande Walle0': 183, 'Heikkinen0': 3, 'Holverson1': 35, 'Sivic0': 471, 'Boulos0': 497, 'Madigan0': 181, 'Abelson1': 277, 'Bowerman1': 312, 'Cohen0': 186, 'Salonen0': 448, 'McDermott0': 80, 'Giglio0': 130, 'McGowan0': 23, 'Smith0': 160, 'Salkjelsvik0': 103, 'Panula5': 50, 'Meek0': 357, 'Sage10': 149, 'Johnson0': 273, 'Dakic0': 560, 'Mockler0': 315, 'Kink-Heilmann2': 168, 'Stead0': 229, 'Berglund0': 207, 'Davies0': 336, 'Lines1': 677, 'Artagaveytia0': 419, 'Jerwan0': 406, 'Connors0': 113, 'Ridsdale0': 446, 'Watt0': 151, 'Davison1': 304, 'Kelly0': 271, 'Padro y Manent0': 459, 'Williams-Lambert0': 308, 'Matthews0': 360, 'Davidson1': 549, 'Dorking0': 256, 'Marechal0': 669, 'Rintamaki0': 492, 'Richards2': 350, 'Willey0': 532, 'Yousseff0': 421, 'Bailey0': 611, 'Jussila0': 483, 'Samaan2': 48, 'Weisz1': 125, 'Van der hoef0': 158, 'Louch1': 373, 'Connolly0': 261, 'Laroche3': 43, 'Cumings1': 2, 'Cleaver0': 576, 'Sobey0': 126, 'Endres0': 581, 'Cunningham0': 355, 'Kenyon1': 392, 'Toomey0': 393, 'Flynn0': 369, 'Fynney0': 21, 'Minahan2': 222, 'Cann0': 37, 'Chronopoulos1': 71, 'Carlsson0': 610, 'Asim0': 318, 'Saalfeld0': 270, 'Sunderland0': 202, 'Coxon0': 91, 'Johannesen-Bratthammer0': 381, 'Beane1': 456, 'Gheorgheff0': 362, 'Spencer1': 31, 'Ostby1': 54, 'Meyer1': 34, 'Greenfield1': 94, 'Christmann0': 87, 'Petterson1': 379, 'Bateman0': 140, 'Phillips0': 368, 'Andrews1': 249, 'Vestrom0': 15, 'Kilgannon0': 629, 'Lennon1': 46, 'Moubarek2': 65, 'van Melkebeke0': 687, 'Kiernan1': 196, 'Gale1': 348, 'Myhrman0': 626, 'Ward0': 235, 'Reuchlin0': 660, 'Jansson0': 341, 'Parrish1': 236, 'Milling0': 398, 'Johanson0': 184, 'Taylor1': 547, 'Hale0': 164, 'Bryhl1': 590, 'Natsch1': 247, 'Madill1': 562, 'Kink2': 68, 'Fleming0': 275, 'Lulic0': 659, 'Jensen1': 584, 'Attalah0': 111, 'Reynaldo0': 380, 'Eklund0': 617, 'Watson0': 552, 'Graham0': 699, 'Gaskell0': 637, 'Devaney0': 44, 'Jenkin0': 69, 'Petroff0': 98, 'Harrington0': 500, 'Lindahl0': 223, 'Hakkarainen1': 133, 'Haas0': 265, 'Troutt0': 582, 'Bazzani0': 200, 'Hoyt0': 638, 'Markun0': 694, 'Nasser1': 10, 'Funk0': 313, 'Ball0': 293, 'Jensen0': 527, 'Tornquist0': 245, 'Jermyn0': 322, 'Olsvigen0': 559, 'Young0': 291, 'Svensson0': 424, 'Brown2': 548, 'Ayoub0': 631, 'Adams0': 346, 'Gee0': 397, 'Mitchell0': 550, 'Dimic0': 306, 'Mellinger1': 246, 'Carr0': 190, 'Moran0': 6, 'Horgan0': 508, 'Oreskovic0': 347, 'Bourke2': 172, 'Tobin0': 627, 'Campbell0': 401, 'Crease0': 67, 'Ilett0': 82, 'Stoytcheff0': 475, 'Sutehall0': 697, \"O'Brien1\": 170, 'Burke0': 134, 'Davies2': 460, 'Colley0': 542, 'Adahl0': 319, 'Mayne0': 577, 'Mionoff0': 102, 'Anderson0': 395, 'Nilsson0': 284, 'Bowen0': 515, 'Slabenoff0': 499, 'Chapman0': 568, 'Clarke1': 367, 'Harper1': 52, 'Byles0': 139, 'de Mulder0': 258, 'Renouf1': 409, 'Hodges0': 586, 'Wilhelms0': 551, 'Shellard0': 423, 'Charters0': 363, 'Kirkland0': 517, 'Stone0': 662, 'Zabour1': 108, 'Kvillner0': 377, 'Najib0': 690, 'Leinonen0': 526, 'Lovell0': 209, 'Waelens0': 78, 'Slemen0': 652, 'Eustis1': 422, 'Lahtinen2': 281, 'Ilmakangas1': 591, 'Harris1': 62, 'Mangan0': 620, 'Kimball1': 513, 'Meo0': 142, 'Turcin0': 173, 'McCarthy0': 7, 'Radeff0': 537, 'Marvin1': 604, 'Canavan0': 425, 'Coleridge0': 220, 'Lobb1': 230, 'Peter2': 120, 'Beckwith2': 225, 'Frauenthal2': 540, 'Hawksford0': 599, 'Tikkanen0': 334, 'Arnold-Franchi1': 49, 'Hansen1': 574, 'Hampe0': 378, 'Barton0': 109, 'Minahan1': 354, 'Sinkkonen0': 603, 'Goldschmidt0': 93, 'Pickard0': 370, 'Burns0': 298, 'Mudd0': 671, 'Andersson6': 14, 'Sjostedt0': 212, 'Robert1': 630, 'Gilinski0': 490, 'Dick1': 563, 'Andersson0': 137, 'Skoog5': 63, 'Baxter1': 114, 'Goldenberg1': 388, 'Reeves0': 240, 'Douglas1': 457, 'de Messemaeker1': 469, 'Widegren0': 349, 'Ohman0': 465, 'Moran1': 106, 'Boulos2': 131, 'Sdycoff0': 352, 'McCoy2': 272, 'Newsom2': 128, 'Molson0': 418, 'Karaic0': 504, 'Tomlin0': 653, 'Hosono0': 260, 'Butt0': 451, 'Sadlier0': 338, 'Vande Velde0': 608, 'Stanley0': 420, 'Razi0': 679, 'Becker3': 167, 'Wells2': 606, 'Cor0': 530, 'Pavlovic0': 440, 'Jonkoff0': 609, 'Saad0': 566, 'Longley0': 518, 'Ross0': 486, 'Petranec0': 97, 'Pasic0': 666, \"O'Connor0\": 394, 'Silverthorne0': 572, 'Sirota0': 667, 'Romaine0': 171, 'Nye0': 66, 'Rosblom2': 231, 'Backstrom3': 83, 'Baclini3': 384, 'Renouf3': 588, 'Ling0': 157, 'Rood0': 169, 'Balkic0': 688, 'Turkula0': 414, 'Millet0': 391, 'Quick2': 429, 'Jacobsohn1': 199, 'Danbom2': 365}\n"
     ]
    }
   ],
   "source": [
    "# First, we'll add titles to the test set.\n",
    "titles = titanic_test[\"Name\"].apply(get_title)\n",
    "# We're adding the Dona title to the mapping, because it's in the test set, but not the training set\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2, \"Dona\": 10}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "titanic_test[\"Title\"] = titles\n",
    "# Check the counts of each unique title.\n",
    "print(pandas.value_counts(titanic_test[\"Title\"]))\n",
    "\n",
    "# Now, we add the family size column.\n",
    "titanic_test[\"FamilySize\"] = titanic_test[\"SibSp\"] + titanic_test[\"Parch\"]\n",
    "\n",
    "# Now we can add family ids.\n",
    "# We'll use the same ids that we did earlier.\n",
    "print(family_id_mapping)\n",
    "\n",
    "family_ids = titanic_test.apply(get_family_id, axis=1)\n",
    "family_ids[titanic_test[\"FamilySize\"] < 3] = -1\n",
    "titanic_test[\"FamilyId\"] = family_ids\n",
    "\n",
    "# The .apply method generates a new series\n",
    "titanic_test[\"NameLength\"] = titanic_test[\"Name\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Predicting On The Test Set\n",
    "We have some better predictions now, so let's create another submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "full_predictions = []\n",
    "for alg, predictors in algorithms:\n",
    "    # Fit the algorithm using the full training data.\n",
    "    alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "    # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
    "    predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    "    full_predictions.append(predictions)\n",
    "\n",
    "# The gradient boosting classifier generates better predictions, so we weight it higher.\n",
    "predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4\n",
    "\n",
    "\n",
    "# Map predictions to outcomes (only possible outcomes are 1 and 0)\n",
    "predictions[predictions > .5] = 1\n",
    "predictions[predictions <=.5] = 0\n",
    "\n",
    "predictions=predictions.astype(int)\n",
    "\n",
    "# Create a new dataframe with only the columns Kaggle wants from the dataset.\n",
    "submission = pandas.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "    \n",
    "submission.to_csv(\"kaggle.csv\", index=False)    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There's still more work you can do in feature engineering:\n",
    "\n",
    "Try using features related to the cabins.\n",
    "See if any family size features might help -- do the number of women in a family make the whole family more likely to survive?\n",
    "Does the national origin of the passenger's name have anything to do with survival?\n",
    "There's also a lot more we can do on the algorithm side:\n",
    "\n",
    "Try the random forest classifier in the ensemble.\n",
    "A support vector machine might work well with this data.\n",
    "We could try neural networks.\n",
    "Boosting with a different base classifier might work better.\n",
    "And with ensembling methods:\n",
    "\n",
    "Could majority voting be a better ensembling method than averaging probabilities?\n",
    "This dataset is very easy to overfit on because there isn't a lot of data, so you'll be grinding for small accuracy gains. You could also try a different Kaggle competition with more data and richer features to sink into."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
